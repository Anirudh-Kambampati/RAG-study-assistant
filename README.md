# ğŸ“„ RAG Study Assistant

A **document-centric Retrieval-Augmented Generation (RAG) application** built with **Streamlit, FAISS, and Ollama** that lets you chat with your documents while keeping answers **strictly grounded in the uploaded content**.

This project evolved through hands-on engineering decisions: managing document ingestion, chunking strategies, vector database lifecycle, chat persistence, and UI state. The result is a **production-style learning system** that is robust, extensible, and suitable for demos or a portfolio.

---

## ğŸ¯ What This Project Solves

Traditional chatbots answer from general knowledge and can hallucinate. This app instead:

- Retrieves **only the most relevant parts of your document**
- Uses those parts as **context for the LLM**
- Produces answers that are **document-scoped and reproducible**
- Keeps **separate chat sessions per document**, so contexts never mix

---

## ğŸ§  What is RAG (Retrieval-Augmented Generation)?

RAG combines **search** and **generation**:

1. Documents are split into chunks
2. Each chunk is converted into an embedding
3. Embeddings are stored in a vector database (FAISS)
4. For every user query:
   - Relevant chunks are retrieved via semantic similarity
   - Retrieved text is passed to the LLM as context
5. The LLM answers **using only that context**

This approach:
- Reduces hallucinations
- Works with private/local data
- Avoids fine-tuning
- Scales better than prompt-stuffing

---

## âœ¨ Key Features

- ğŸ“ Upload documents (`PDF`, `DOCX`, `PPTX`, `TXT`)
- ğŸ’¬ Chat-style UI (ChatGPT-like experience)
- ğŸ§  True Retrieval-Augmented Generation
- ğŸ” FAISS-based semantic search
- ğŸ“š Persistent chat history per document
- ğŸ—‚ï¸ One chat per document (no cross-contamination)
- ğŸ§­ Multi-page Streamlit application
- ğŸ–¥ï¸ Fully local inference using Ollama (no API keys)
- â™»ï¸ Controlled vector database lifecycle to avoid disk/RAM bloat

---

## ğŸ—ï¸ System Architecture

```

User
â”‚
â–¼
Streamlit UI (Multi-page)
â”‚
â”œâ”€â”€ New Chat / Upload Page
â”œâ”€â”€ Chats List Page
â”œâ”€â”€ Chat Page (per document)
â”‚
â–¼
FAISS Vector Store (per document)
â”‚
â–¼
Ollama Local LLM

```

### Design Principles

- **One document = one chat**
- **One document = one FAISS index**
- **Vector DBs are treated as cache, not source code**
- **Chat history is persistent and reloadable**
- **UI state is cleanly separated from data storage**

---

## ğŸ—‚ï¸ Project Structure

```

rag-project/
â”‚
â”œâ”€â”€ loaders/              # Document loaders (PDF, DOCX, PPTX, TXT)
â”œâ”€â”€ chunking/             # Text splitting & chunking logic
â”œâ”€â”€ embeddings/           # Embedding model wrapper
â”œâ”€â”€ vector_store/         # FAISS index creation & loading
â”œâ”€â”€ llm/                  # Ollama LLM wrapper
â”‚
â”œâ”€â”€ pages/                # Streamlit multipage UI
â”‚   â”œâ”€â”€ 1_Chats.py        # List & reopen previous chats
â”‚   â””â”€â”€ 2_Chat.py         # Chat interface
â”‚
â”œâ”€â”€ chat_store.json       # Persistent chat metadata
â”œâ”€â”€ chat_utils.py         # Chat persistence utilities
â”œâ”€â”€ ui.py                 # Main entry point (new chat / upload)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

````

---

## âš™ï¸ Installation & Setup

This application is designed to run **locally**.

It relies on:
- **Ollama** for local LLM inference
- **FAISS** for vector search
- Native document processing libraries

Because of these dependencies, it is **not suitable for direct deployment** on platforms like:
- Streamlit Cloud
- Vercel
- Netlify

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/Anirudh-Kambampati/RAG-study-assistant.git
cd RAG-study-assistant
````

---

### 2ï¸âƒ£ Create a Virtual Environment

```bash
python -m venv .venv
```

Activate it:

**Windows**

```bash
.venv\Scripts\activate
```

**Linux / macOS**

```bash
source .venv/bin/activate
```

---

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

The dependency list is intentionally **defensive**, covering all loaders and edge cases encountered during development.

---

## ğŸ§  Ollama Setup (Required)

This project uses **local LLM inference** via Ollama.

### Install Ollama

ğŸ‘‰ [https://ollama.com](https://ollama.com)

### Pull Required Models

```bash
ollama pull phi3:mini
ollama pull nomic-embed-text
```

> âš ï¸ For low-RAM systems, consider smaller models such as `phi` or `mistral`, since my system is low ram, the default is set to phi3:mini 
---

## ğŸš€ Running the Application

```bash
streamlit run ui.py
```

Open your browser at:

```
http://localhost:8501
```

---

## ğŸ§ª How the Application Works (Step-by-Step)

1. User uploads a document
2. Document is parsed and cleaned
3. Text is split into overlapping, context-aware chunks
4. Chunks are embedded and stored in FAISS
5. User asks a question
6. FAISS retrieves the most relevant chunks
7. Retrieved context is sent to the LLM
8. The LLM generates a grounded response

The model **never answers without retrieved document context**.

---

## ğŸ—‚ï¸ Chat System Design

* Each document has:

  * its own FAISS index
  * its own chat history
* Chat metadata is stored in `chat_store.json`
* Chats persist across:

  * page reloads
  * browser refreshes
  * application restarts
* Old chats can be reopened and continued seamlessly

---

## ğŸ§¹ Repository Hygiene

The following are **intentionally not committed**:

* `.venv/`
* FAISS index data
* Uploaded documents
* OS cache files

These are runtime artifacts and should not be version-controlled.

---

## ğŸ› ï¸ Tech Stack

* **Python**
* **Streamlit**
* **LangChain**
* **FAISS**
* **Ollama**
* **Unstructured**
* **PyPDF**
* **python-docx**
* **python-pptx**

---
## ğŸ‘¤ Author

**Anirudh Kambampati**

## ğŸ“œ License

MIT License. See the [LICENSE](LICENSE) file for more information.
